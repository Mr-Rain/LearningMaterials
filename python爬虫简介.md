# python爬虫简介

## 一、爬虫原理

什么是爬虫？爬虫就是获取网页并提取和保存信息的自动化程序。一个爬虫基本要分为以下几个步骤：

### 1. 获取网页

爬虫首先要做的工作就是获取网页，在这里获取网页即获取网页的源代码，源代码里面必然包含了网页的部分有用的信息，所以只要把源代码获取下来了，就可以从中提取我们想要的信息了。

我们向网站的服务器发送一个 Request，返回的 Response 的 Body 便是网页源代码。所以最关键的部分就是构造一个 Request 并发送给服务器，然后接收到 Response 并将其解析出来。Python 里面提供了许多库来帮助我们实现这个操作，如 Urllib、Requests 等，我们可以用这些库来帮助我们实现 HTTP 请求操作，Request 和 Response 都可以用类库提供的数据结构来表示，得到 Response 之后只需要解析数据结构中的 Body 部分即可，即得到网页的源代码，这样我们可以用程序来实现获取网页的过程了。

### 2. 提取信息

我们在第一步获取了网页源代码之后，接下来的工作就是分析网页源代码，从中提取我们想要的数据，首先最通用的方法便是采用正则表达式提取，这是一个万能的方法，但是在构造正则表达式的时候比较复杂且容易出错。

另外由于网页的结构是有一定规则的，所以还有一些根据网页节点属性、CSS 选择器或 XPath 来提取网页信息的库，如 BeautifulSoup、PyQuery、LXML 等，使用这些库可以高效快速地从中提取网页信息，如节点的属性、文本值等内容。

提取信息是爬虫非常重要的部分，它可以使杂乱的数据变得清晰条理，以便于我们后续在对数据进行处理和分析。

### 3. 保存数据

提取信息之后我们一般会将提取到的数据保存到某处以便后续数据处理使用。保存形式有多种多样，如可以简单保存为 TXT 文本或 Json 文本，也可以保存到数据库，如 MySQL、MongoDB 等，也可保存至远程服务器，如借助 Sftp 进行操作等。

### 4.  数据抓取

在网页中我们能看到各种各样的信息，最常见的便是常规网页，其都对应着 HTML 代码，而最常见的抓取便是抓取 HTML 源代码。

另外可能有些网页返回的不是 HTML 代码，而是返回一个 Json 字符串，API 接口大多采用这样的形式，方便数据的传输和解析，这种数据同样可以抓取，而且数据提取更加方便。

此外我们还可以看到各种二进制数据，如图片、视频、音频等等，我们可以利用爬虫将它们的二进制数据抓取下来，然后保存成对应的文件名即可。

另外我们还可以看到各种扩展名的文件，如 CSS、JavaScript、配置文件等等，这些其实也是最普通的文件，只要在浏览器里面访问到，我们就可以将其抓取下来。

以上的内容其实都对应着各自的URL，是基于 HTTP 或 HTTPS 协议的，只要是这种数据爬虫都可以进行抓取。但是现在大部分网页都是动态网页，采用Ajax进行交互，这时我们所能爬到的并不包括js文件。对于这样的情况，我们可以分析其后台 Ajax 接口，也可使用 Selenium、Splash 这样的库来实现模拟 JavaScript 渲染，这样我们便可以爬取 JavaScript 渲染的网页的内容了。

## 二、网页获取

